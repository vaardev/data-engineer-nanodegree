{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Study of Immigration and Temperature Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "\n",
    "Scope the Project and Gather Data\n",
    "\n",
    "##Scope\n",
    "\n",
    "The goal is to create an ETL pipeline using the Udacity provided I94 immigration dataset and the city temperature data from Kaggle to allow users to make queries to see and assess if there is a correlation between destination temperature and immigration statistics.\n",
    "\n",
    "##Potential Consummers of this data model:\n",
    "\n",
    "Data Analysts and Data Scientists are primary consumers of this data model in order to apply exploration techniques (data mining) which will help them to understand the potential correlation between destination temperature and immigration statistics.\n",
    "\n",
    "This data model could be integrate into a Data Visualization tool (e.g.PoweBI, Tableau) and BI analyst could explore and apply different exploration technics;\n",
    "\n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "In this project, we will be looking at the immigration data, focusing on following hypothesis:\n",
    "\n",
    "-> The effects and correlation between temperature on the volume of travellers;\n",
    "\n",
    "-> Travel seasonality\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "1.The I94 immigration data comes from the US National Tourism and Trade Office. It is provided in SAS specific format(SAS7BDAT) which is a database storage format. Some relevant attributes include:\n",
    "\n",
    "i94yr         = 4 digit year\n",
    "i94mon        = numeric month\n",
    "i94cit        = 3 digit code of origin city\n",
    "i94port       = 3 character code of destination USA city\n",
    "arrdate       = arrival date in the USA\n",
    "i94mode       = 1 digit travel code\n",
    "depdate       = departure date from the USA\n",
    "i94visa       = reason for immigration\n",
    "\n",
    "\n",
    "2.The temperature data comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "AverageTemperature = average temperature\n",
    "City               = city name\n",
    "Country            = country name\n",
    "Latitude           = latitude\n",
    "Longitude          = longitude\n",
    "dt                 = Date\n",
    "\n",
    "\n",
    "3.Airport Code Table: This is a simple table of airport codes and corresponding cities. The airport codes may refer to following types:\n",
    "-> IATA airport code which consists in three-letter code which is used in passenger reservation, ticketing and baggage-handling systems;\n",
    "-> ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia). It comes from  - > https://datahub.io/core/airport-codes#data\n",
    "\n",
    "ident        = Unique identifier\n",
    "Name         = Name of Airport\n",
    "Type         = Type of Airport (e.g.helliport, small_airport etc.)\n",
    "Elevation_ft = Elevation of airport\n",
    "iso_country  = Airport country location\n",
    "iso_region   = Airport region\n",
    "Coordinates  = Airport coordinates\n",
    "municipality = City where the airport is located\n",
    "gps_code     = GPS code of the airport\n",
    "iata_code    = IATA code of the airport\n",
    "local_code   = Local code of the airport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read April 2016 I94 immigration data into Pandas for exploration\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read temperature data into Pandas for exploration\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few entries of temperature data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Airports location into Pandas for exploration\n",
    "fname = 'airport_location.csv'\n",
    "df = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few entries of Airports Location Info\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "We are not using the airport dataset in our model. We draw the conclusion that it did not prove to be a good source of analysis once we were not able to join this to the main table immigration in a consistent and efficient way.\n",
    "\n",
    "We did not find a valid and consistent key in both tables in order to cross them. None of the codes (ident, gps_code, iata_code or local_code) are quite redundant to the information we already have and in case we use iata_code as merging key we'll get information only for main airports and more than 70% from airports resided in this table will not be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Data Description & Sources;\n",
    "\n",
    "1.I94 Immigration Data: This data comes from the US National Tourism and Trade Office found here. Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups etc.;\n",
    "\n",
    "2.World Temperature Data: This dataset came from Kaggle;\n",
    "\n",
    "3.Airport Code Table: This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia). It comes from  - > https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Quality assessment\n",
    "\n",
    "I've initiated following steps in order to clean both main data sources:\n",
    "\n",
    "1.I94 immigration data:\n",
    "    The main objectif is to handdle all entries where the destination city code i94port is not a valid value (e.g., XXX ) as described in I94_SAS_Labels_Description.SAS.\n",
    " \n",
    "2. Temperature Data:\n",
    "    2.1 Clean all entries where AverageTemperature attributes has NaN value;\n",
    "    2.2 Treat all entries with duplicate locations;\n",
    "    2.3 Finally, we need to include i94port attribute in each entry;\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# I94 immigration data quality assessment\n",
    "# City airport codes are cleaned using the txt file with airport_codes.txt (source:https://en.wikipedia.org/wiki/IATA_airport_code)\n",
    "\n",
    "\n",
    "# Create dictionary of valid i94port codes\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "port_valid = {}\n",
    "with open('airport_codes.txt') as f:\n",
    "     for line in f:\n",
    "         match = re_obj.search(line)\n",
    "         port_valid[match[1]]=[match[2]]\n",
    "\n",
    "def clean_i94_data(file):\n",
    "    '''\n",
    "    Input: Path to I94 immigration data file\n",
    "    \n",
    "    Output: Spark dataframe of I94 immigration data with valid airport codes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read I94 data into Spark\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "    # Filter out entries where i94port is invalid\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(port_valid.keys())))\n",
    "\n",
    "    return df_immigration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20561.0|  37.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1979.0|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test function\n",
    "immigration_test_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "df_immigration_test = clean_i94_data(immigration_test_file)\n",
    "df_immigration_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a view of the immigration dataset in order to assess key attributes from this source\n",
    "df_immigration_test.createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3088544"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|len|\n",
      "+---+\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check if the airport codes in the dataset having all observation 3 character long\n",
    "spark.sql(\"\"\"\n",
    "SELECT LENGTH (i94port) AS len\n",
    "FROM immig_table\n",
    "GROUP BY len\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Temperature data quality assessment\n",
    "df_temp=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "\n",
    "# Isolate and Filter out observations with NaN average temperature\n",
    "df_temp=df_temp.filter(df_temp.AverageTemperature != 'NaN')\n",
    "\n",
    "# Remove duplicates on locations attribute\n",
    "df_temp=df_temp.dropDuplicates(['City', 'Country'])\n",
    "\n",
    "@udf()\n",
    "def get_i94port(city):\n",
    "    '''\n",
    "    Input: City name\n",
    "    \n",
    "    Output: Mapping with i94port attribute\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for key in port_valid:\n",
    "        if city.lower() in port_valid[key][0].lower():\n",
    "            return key\n",
    "\n",
    "# Include iport94 code based on city name\n",
    "df_temp=df_temp.withColumn(\"i94port\", get_i94port(df_temp.City))\n",
    "\n",
    "# Remove entries with no iport94 code\n",
    "df_temp=df_temp.filter(df_temp.i94port != 'null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dt='1852-07-01', AverageTemperature='15.488', AverageTemperatureUncertainty='1.395', City='Perth', Country='Australia', Latitude='31.35S', Longitude='114.97E', i94port='PER'),\n",
       " Row(dt='1828-01-01', AverageTemperature='-1.977', AverageTemperatureUncertainty='2.551', City='Seattle', Country='United States', Latitude='47.42N', Longitude='121.97W', i94port='SEA'),\n",
       " Row(dt='1743-11-01', AverageTemperature='2.767', AverageTemperatureUncertainty='1.905', City='Hamilton', Country='Canada', Latitude='42.59N', Longitude='80.73W', i94port='HAM'),\n",
       " Row(dt='1849-01-01', AverageTemperature='7.399999999999999', AverageTemperatureUncertainty='2.699', City='Ontario', Country='United States', Latitude='34.56N', Longitude='116.76W', i94port='ONT'),\n",
       " Row(dt='1821-11-01', AverageTemperature='2.322', AverageTemperatureUncertainty='2.375', City='Spokane', Country='United States', Latitude='47.42N', Longitude='117.24W', i94port='SPO'),\n",
       " Row(dt='1843-01-01', AverageTemperature='18.874000000000002', AverageTemperatureUncertainty='2.017', City='Abu Dhabi', Country='United Arab Emirates', Latitude='24.92N', Longitude='54.98E', i94port='MAA'),\n",
       " Row(dt='1824-01-01', AverageTemperature='25.229', AverageTemperatureUncertainty='1.094', City='Anaco', Country='Venezuela', Latitude='8.84N', Longitude='64.05W', i94port='ANA'),\n",
       " Row(dt='1855-05-01', AverageTemperature='9.904', AverageTemperatureUncertainty='1.4369999999999998', City='Ica', Country='Peru', Latitude='13.66S', Longitude='75.14W', i94port='CHI'),\n",
       " Row(dt='1835-01-01', AverageTemperature='9.833', AverageTemperatureUncertainty='2.182', City='Nogales', Country='United States', Latitude='31.35N', Longitude='111.20W', i94port='NOG'),\n",
       " Row(dt='1743-11-01', AverageTemperature='8.129999999999999', AverageTemperatureUncertainty='2.245', City='Atlanta', Country='United States', Latitude='34.56N', Longitude='83.68W', i94port='ATL'),\n",
       " Row(dt='1796-01-01', AverageTemperature='15.552', AverageTemperatureUncertainty='2.305', City='Mau', Country='India', Latitude='26.52N', Longitude='84.18E', i94port='OGG'),\n",
       " Row(dt='1743-11-01', AverageTemperature='3.264', AverageTemperatureUncertainty='1.665', City='Newark', Country='United States', Latitude='40.99N', Longitude='74.56W', i94port='NEW'),\n",
       " Row(dt='1857-01-01', AverageTemperature='18.581000000000003', AverageTemperatureUncertainty='1.8119999999999998', City='Springs', Country='South Africa', Latitude='26.52S', Longitude='28.66E', i94port='PSP'),\n",
       " Row(dt='1856-01-01', AverageTemperature='26.055999999999997', AverageTemperatureUncertainty='1.3769999999999998', City='Ise', Country='Nigeria', Latitude='7.23N', Longitude='5.68E', i94port='BOI'),\n",
       " Row(dt='1743-11-01', AverageTemperature='18.722', AverageTemperatureUncertainty='2.302', City='Orlando', Country='United States', Latitude='28.13N', Longitude='80.91W', i94port='ORL'),\n",
       " Row(dt='1823-01-01', AverageTemperature='11.602', AverageTemperatureUncertainty='2.8160000000000003', City='Laredo', Country='United States', Latitude='28.13N', Longitude='99.09W', i94port='LCB'),\n",
       " Row(dt='1841-01-01', AverageTemperature='13.107999999999999', AverageTemperatureUncertainty='2.519', City='Tali', Country='Taiwan', Latitude='24.92N', Longitude='120.59E', i94port='MET'),\n",
       " Row(dt='1828-01-01', AverageTemperature='-2.7630000000000003', AverageTemperatureUncertainty='2.617', City='Victoria', Country='Canada', Latitude='49.03N', Longitude='122.45W', i94port='VIC'),\n",
       " Row(dt='1743-11-01', AverageTemperature='1.1880000000000002', AverageTemperatureUncertainty='1.531', City='Boston', Country='United States', Latitude='42.59N', Longitude='72.00W', i94port='BOS'),\n",
       " Row(dt='1849-01-01', AverageTemperature='8.091999999999999', AverageTemperatureUncertainty='2.1919999999999997', City='Fairfield', Country='United States', Latitude='37.78N', Longitude='122.03W', i94port='FTF')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Quality assessment function\n",
    "df_temp.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(fname)\n",
    "df_temperature['Country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values.\n",
    "df_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1745-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1745-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1745-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1745-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1745-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1745-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1745-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1745-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1746-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1746-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1746-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1746-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1746-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1746-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1746-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1746-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1746-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1746-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1746-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1746-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1747-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1747-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1747-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1747-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1747-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596029</th>\n",
       "      <td>1748-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596030</th>\n",
       "      <td>1748-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596031</th>\n",
       "      <td>1748-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596032</th>\n",
       "      <td>1748-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596033</th>\n",
       "      <td>1748-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596034</th>\n",
       "      <td>1748-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596035</th>\n",
       "      <td>1749-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596036</th>\n",
       "      <td>1749-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596037</th>\n",
       "      <td>1749-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596038</th>\n",
       "      <td>1749-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596039</th>\n",
       "      <td>1749-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596040</th>\n",
       "      <td>1749-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596041</th>\n",
       "      <td>1749-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596042</th>\n",
       "      <td>1749-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596043</th>\n",
       "      <td>1749-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596044</th>\n",
       "      <td>1749-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596045</th>\n",
       "      <td>1749-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596046</th>\n",
       "      <td>1749-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596057</th>\n",
       "      <td>1750-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596063</th>\n",
       "      <td>1751-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596068</th>\n",
       "      <td>1751-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596069</th>\n",
       "      <td>1751-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596070</th>\n",
       "      <td>1751-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596072</th>\n",
       "      <td>1752-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596075</th>\n",
       "      <td>1752-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596076</th>\n",
       "      <td>1752-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596077</th>\n",
       "      <td>1752-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596078</th>\n",
       "      <td>1752-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596079</th>\n",
       "      <td>1752-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364130 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "1        1743-12-01                 NaN                            NaN   \n",
       "2        1744-01-01                 NaN                            NaN   \n",
       "3        1744-02-01                 NaN                            NaN   \n",
       "4        1744-03-01                 NaN                            NaN   \n",
       "9        1744-08-01                 NaN                            NaN   \n",
       "18       1745-05-01                 NaN                            NaN   \n",
       "19       1745-06-01                 NaN                            NaN   \n",
       "20       1745-07-01                 NaN                            NaN   \n",
       "21       1745-08-01                 NaN                            NaN   \n",
       "22       1745-09-01                 NaN                            NaN   \n",
       "23       1745-10-01                 NaN                            NaN   \n",
       "24       1745-11-01                 NaN                            NaN   \n",
       "25       1745-12-01                 NaN                            NaN   \n",
       "26       1746-01-01                 NaN                            NaN   \n",
       "27       1746-02-01                 NaN                            NaN   \n",
       "28       1746-03-01                 NaN                            NaN   \n",
       "29       1746-04-01                 NaN                            NaN   \n",
       "30       1746-05-01                 NaN                            NaN   \n",
       "31       1746-06-01                 NaN                            NaN   \n",
       "32       1746-07-01                 NaN                            NaN   \n",
       "33       1746-08-01                 NaN                            NaN   \n",
       "34       1746-09-01                 NaN                            NaN   \n",
       "35       1746-10-01                 NaN                            NaN   \n",
       "36       1746-11-01                 NaN                            NaN   \n",
       "37       1746-12-01                 NaN                            NaN   \n",
       "38       1747-01-01                 NaN                            NaN   \n",
       "39       1747-02-01                 NaN                            NaN   \n",
       "40       1747-03-01                 NaN                            NaN   \n",
       "41       1747-04-01                 NaN                            NaN   \n",
       "42       1747-05-01                 NaN                            NaN   \n",
       "...             ...                 ...                            ...   \n",
       "8596029  1748-07-01                 NaN                            NaN   \n",
       "8596030  1748-08-01                 NaN                            NaN   \n",
       "8596031  1748-09-01                 NaN                            NaN   \n",
       "8596032  1748-10-01                 NaN                            NaN   \n",
       "8596033  1748-11-01                 NaN                            NaN   \n",
       "8596034  1748-12-01                 NaN                            NaN   \n",
       "8596035  1749-01-01                 NaN                            NaN   \n",
       "8596036  1749-02-01                 NaN                            NaN   \n",
       "8596037  1749-03-01                 NaN                            NaN   \n",
       "8596038  1749-04-01                 NaN                            NaN   \n",
       "8596039  1749-05-01                 NaN                            NaN   \n",
       "8596040  1749-06-01                 NaN                            NaN   \n",
       "8596041  1749-07-01                 NaN                            NaN   \n",
       "8596042  1749-08-01                 NaN                            NaN   \n",
       "8596043  1749-09-01                 NaN                            NaN   \n",
       "8596044  1749-10-01                 NaN                            NaN   \n",
       "8596045  1749-11-01                 NaN                            NaN   \n",
       "8596046  1749-12-01                 NaN                            NaN   \n",
       "8596057  1750-11-01                 NaN                            NaN   \n",
       "8596063  1751-05-01                 NaN                            NaN   \n",
       "8596068  1751-10-01                 NaN                            NaN   \n",
       "8596069  1751-11-01                 NaN                            NaN   \n",
       "8596070  1751-12-01                 NaN                            NaN   \n",
       "8596072  1752-02-01                 NaN                            NaN   \n",
       "8596075  1752-05-01                 NaN                            NaN   \n",
       "8596076  1752-06-01                 NaN                            NaN   \n",
       "8596077  1752-07-01                 NaN                            NaN   \n",
       "8596078  1752-08-01                 NaN                            NaN   \n",
       "8596079  1752-09-01                 NaN                            NaN   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "           City      Country Latitude Longitude  \n",
       "1         Århus      Denmark   57.05N    10.33E  \n",
       "2         Århus      Denmark   57.05N    10.33E  \n",
       "3         Århus      Denmark   57.05N    10.33E  \n",
       "4         Århus      Denmark   57.05N    10.33E  \n",
       "9         Århus      Denmark   57.05N    10.33E  \n",
       "18        Århus      Denmark   57.05N    10.33E  \n",
       "19        Århus      Denmark   57.05N    10.33E  \n",
       "20        Århus      Denmark   57.05N    10.33E  \n",
       "21        Århus      Denmark   57.05N    10.33E  \n",
       "22        Århus      Denmark   57.05N    10.33E  \n",
       "23        Århus      Denmark   57.05N    10.33E  \n",
       "24        Århus      Denmark   57.05N    10.33E  \n",
       "25        Århus      Denmark   57.05N    10.33E  \n",
       "26        Århus      Denmark   57.05N    10.33E  \n",
       "27        Århus      Denmark   57.05N    10.33E  \n",
       "28        Århus      Denmark   57.05N    10.33E  \n",
       "29        Århus      Denmark   57.05N    10.33E  \n",
       "30        Århus      Denmark   57.05N    10.33E  \n",
       "31        Århus      Denmark   57.05N    10.33E  \n",
       "32        Århus      Denmark   57.05N    10.33E  \n",
       "33        Århus      Denmark   57.05N    10.33E  \n",
       "34        Århus      Denmark   57.05N    10.33E  \n",
       "35        Århus      Denmark   57.05N    10.33E  \n",
       "36        Århus      Denmark   57.05N    10.33E  \n",
       "37        Århus      Denmark   57.05N    10.33E  \n",
       "38        Århus      Denmark   57.05N    10.33E  \n",
       "39        Århus      Denmark   57.05N    10.33E  \n",
       "40        Århus      Denmark   57.05N    10.33E  \n",
       "41        Århus      Denmark   57.05N    10.33E  \n",
       "42        Århus      Denmark   57.05N    10.33E  \n",
       "...         ...          ...      ...       ...  \n",
       "8596029  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596030  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596031  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596032  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596033  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596034  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596035  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596036  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596037  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596038  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596039  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596040  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596041  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596042  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596043  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596044  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596045  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596046  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596057  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596063  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596068  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596069  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596070  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596072  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596075  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596076  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596077  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596078  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8596079  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599211  Zwolle  Netherlands   52.24N     5.26E  \n",
       "\n",
       "[364130 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature[df_temperature.AverageTemperature.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "3.1 Data Model - concept\n",
    "\n",
    "3.1.1 Dimension tables\n",
    "\n",
    "The first dimension table will contain events from the I94 immigration data. The attributes will be extracted from the immigration dataframe:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "i94yr             =  Year (4 digits)\n",
    "i94mon            =  numeric month\n",
    "i94cit            =  Origin City (3 digits code)\n",
    "i94port           =  Destination City (3 character code)\n",
    "arrdate           =  Date of Arrival\n",
    "i94mode           =  Travel Code (1 digit)\n",
    "depdate           =  Date of departure\n",
    "i94visa           =  Immigration Reason\n",
    "visatype          =  Type of Visa\n",
    "Gender            =  Sex\n",
    "\n",
    "The second dimension table will contain city temperature data. The attributes will be extracted from the temperature dataframe:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "i94port             = Destination city (mapped from immigration data during cleanup step) - 3 char code\n",
    "AverageTemperature  = average temperature\n",
    "City                = Name of the city\n",
    "Country             = Name of the country\n",
    "Latitude            = Latitude\n",
    "Longitude           = Longitude\n",
    "\n",
    "\n",
    "dim_time_table\n",
    "\n",
    "Get all the arrival dates from the immigration dataset;\n",
    "extract year, month, day, week from the date and insert all the values in the dim_time_table;\n",
    "\n",
    "Write to parquet\n",
    "\n",
    "3.1.2 Fact table\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data and airport location on i94port:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "i94yr              = Year(4 digits)\n",
    "i94mon             = numeric month\n",
    "i94cit             = Origin city(3 digits code)\n",
    "i94port            = Destination city(3 characters code)\n",
    "arrdate            = Date of arrival\n",
    "i94mode            = Travel Code (1 digit)\n",
    "depdate            = Date of Departure\n",
    "i94visa            = immigration reason\n",
    "AverageTemperature = Average temperature of destination city\n",
    "visatype           = Type of visa\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city (i94port attribute) except dim_time_table.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "The pipeline steps are described below:\n",
    "\n",
    "1. Initiate quality assessment for I94 attribute data as described in step 2 to create Spark dataframe df_immigration for each month;\n",
    "\n",
    "2. Initiate quality assessment for temperature data as described in step 2 to create Spark dataframe df_temp (already performed);\n",
    "\n",
    "3. Create immigration dimension table by selecting relevant columns from df_immigration and write to parquet file partitioned by i94port;\n",
    "\n",
    "4. Create temperature dimension table by selecting relevant columns from df_temp and write to parquet file partitioned by i94port;\n",
    "\n",
    "5. Create time dimension table by extracting year, month, day, week from the date (Arrival Date from immigration dimension table) write to parquet file;\n",
    "\n",
    "6. Create fact table by joining immigration and temperature dimension tables on i94port and write to parquet file partitioned by i94port;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to I94 immigration data \n",
    "immigration_data = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "# Initiate quality assessment for I94 attribute data and store as Spark dataframe\n",
    "df_immigration = clean_i94_data(immigration_data)\n",
    "\n",
    "# Extract columns for immigration dimension table\n",
    "dim_immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\", \"visatype\", \"Gender\"])\n",
    "\n",
    "# Write immigration dimension table to parquet files partitioned by i94port\n",
    "dim_immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "dim_temp_table = df_temp.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])\n",
    "\n",
    "# Write temperature dimension table to parquet files partitioned by i94port\n",
    "dim_temp_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temporary views of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immig_view\")\n",
    "df_temp.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows where the gender values entered is undefined\n",
    "spark.sql(\"\"\"SELECT * FROM immig_view WHERE gender IN ('F', 'M')\"\"\").createOrReplaceTempView(\"immig_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the departure dates into a useable value\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN arrdate >= 1.0 THEN date_add(to_date('1960-01-01'), arrdate)\n",
    "                        WHEN arrdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS arrival_date \n",
    "                        \n",
    "                FROM immig_view\"\"\").createOrReplaceTempView(\"immig_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the departure dates into a useable value\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date \n",
    "                        \n",
    "                FROM immig_view\"\"\").createOrReplaceTempView(\"immig_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# extract all distinct dates from arrival and departure dates to create dimension table\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT arrival_date AS date\n",
    "FROM immig_view\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM immig_view\n",
    "WHERE departure_date IS NOT NULL\n",
    "\"\"\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract year, month, day, weekofyear, dayofweek and weekofyear from the date and insert all the values in the dim_time table;\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT date, YEAR(date) AS year, MONTH(date) AS month, DAY(date) AS day, WEEKOFYEAR(date) AS week, DAYOFWEEK(date) as weekday, DAYOFYEAR(date) year_day\n",
    "FROM dim_time_table\n",
    "ORDER BY date ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract year, month, day, weekofyear, dayofweek and weekofyear from the date and insert all the values in the dim_time table;\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT date, YEAR(date) AS year, MONTH(date) AS month, DAY(date) AS day, WEEKOFYEAR(date) AS week, DAYOFWEEK(date) as weekday, DAYOFYEAR(date) year_day\n",
    "FROM dim_time_table\n",
    "ORDER BY date ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Saving the data in parquet format\n",
    "dim_time.write.parquet(\"/results/dim_time.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql('''\n",
    "SELECT immig_view.i94yr as year,\n",
    "       immig_view.i94mon as month,\n",
    "       immig_view.i94cit as city,\n",
    "       immig_view.i94port as i94port,\n",
    "       immig_view.arrival_date as arrival_date,\n",
    "       immig_view.departure_date as departure_date,\n",
    "       immig_view.i94visa as reason,\n",
    "       immig_view.Gender as Gender,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "       temp_view.Latitude as latitude,\n",
    "       temp_view.Longitude as longitude\n",
    "FROM immig_view\n",
    "JOIN temp_view ON (immig_view.i94port = temp_view.i94port)\n",
    "''')\n",
    "\n",
    "# Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration table with 3088544 records\n",
      "Data quality check passed for temperature table with 207 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input:  Description of Spark datafram\n",
    "    \n",
    "    Output: Data qulity check results\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0\n",
    "\n",
    "# Perform data quality check\n",
    "quality_check(df_immigration, \"immigration table\")\n",
    "quality_check(df_temp, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "i94yr             =  Year (4 digits)\n",
    "i94mon            =  numeric month\n",
    "i94cit            =  Origin City (3 digits code)\n",
    "i94port           =  Destination City (3 character code)\n",
    "Arrival_date      =  Date of Arrival\n",
    "i94mode           =  Travel Code (1 digit)\n",
    "Departure_date    =  Date of departure\n",
    "i94visa           =  Immigration Reason\n",
    "visatype          =  Type of Visa\n",
    "Gender            =  Sex\n",
    "\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "i94port             = Destination city (mapped from immigration data during cleanup step) - 3 char code\n",
    "AverageTemperature  = average temperature\n",
    "City                = Name of the city\n",
    "Country             = Name of the country\n",
    "Latitude            = Latitude\n",
    "Longitude           = Longitude\n",
    "\n",
    "The third dimension table will contain all the arrival dates from the immigration dataset extracting year, month, day, week from the date and insert all the values in the dim_time_table;\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "year               = Year (From Arrival Date)\n",
    "month              = Month (From Arrival Date)\n",
    "week               = Week (From Arrival Date)\n",
    "weekday            = Weekday (From Arrival Date)\n",
    "year_day           = Day in Year (From Arrival Date)\n",
    "\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "Attribute Name       Description\n",
    "\n",
    "year           =  Year (4 digits)\n",
    "month          =  numeric month\n",
    "city           =  Origin City (3 digits code)\n",
    "i94port        =  Destination City (3 character code)\n",
    "arrival_date   =  Date of Arrival\n",
    "departure_date =  Date of departure\n",
    "reason         =  Immigration Reason\n",
    "Gender         =  Sex\n",
    "temperature    =  Average temperature\n",
    "latitude       =  Latitude\n",
    "longitude      =  Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Purpose of data model;\n",
    "\n",
    "The purpose of this data model is to provide a consistent source of information from which different analytics teams could initiate followings:\n",
    "\n",
    "->Data exploration;(cluster, correlations, retrospective analysis)\n",
    "->Find potential correlations between destination temperature and immigration statistics;\n",
    "->Travel sesonality;\n",
    "->Understand the traveller behaviour and initiate classifications;\n",
    "\n",
    "Primary consumers of this data model are followings:\n",
    "\n",
    "Data Scientist;\n",
    "Data Analyst;\n",
    "BI analyst;\n",
    "\n",
    "Data is saved in parquet which has the following advantages:\n",
    "->Size efficiency which drives sustainability in case data will increase;\n",
    "->I/O failures are minimized;\n",
    "\n",
    "Data model could be connected to a Data Visualization tool such as Power BI or Tableau;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "The main purpose of this data model is to be a consistent and efficient source of data for consumers such as Data Scientists, Data Analysts or BI analysts for following use cases:\n",
    "\n",
    "1.Data exploration (correlations using attributes from fact table e.g. temperature and Arrival date or Destination City)\n",
    "\n",
    "    1.1 Applying clustering techniques in order to assess geographical clusters per different continuos/discrete variables (temperature, reason etc.) - The consumer of data model needs to initiate a query to aggregate data at Destinaty City level;\n",
    "    \n",
    "2.Travel sesonality - another potential use case which analytics team could cover using this data model is to assess the travel seasonality in understanding frequency of flights over the year;\n",
    "\n",
    "    ->Analytics team could take a long time period of time (3-5 years) in order to draw consistent conlusions;\n",
    "    ->Analytics team needs to query finalized years and use month or day (depending on granularity of seasonality analysis);\n",
    "\n",
    "3.Traveler behavior - Analytics team could initiate queries in order to define traveler behaviour;\n",
    "\n",
    "    ->In this regard they need to take into consideration explanatory variables such as : Gender, Immigation Reason, Origin City etc.\n",
    "    \n",
    "Technologies used:\n",
    "\n",
    "Spark was used as main tool in creating data model due to below reasons:\n",
    "\n",
    "->Imigration Dataset has a considerable amount of data (~ 3 million rows) combined with temperature date;\n",
    "\n",
    "->We need to drive sustainability to process data over a longer period of time;\n",
    "\n",
    "->Drive resilience in case we want to integrate new external sources;\n",
    "\n",
    "->Ease the work to create new table derived from existing tables (dim_time_table);\n",
    "\n",
    "->Posibility to process multiple queries quickly which drives to time efficient to explore, quality assess and create the data model;\n",
    "\n",
    "->Spark is easing our work in handling multiple types of formats (such as SAS format)\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "We stated at the beginning of this project that we were interested in:\n",
    "\n",
    "-> The effects and correlation between temperature on the volume of travellers;\n",
    "\n",
    "-> Travel seasonality\n",
    "\n",
    "None of these phenomenons require a rapid update of our data.The data update in order to cover the needs of this study would be monthly or quarterly.\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " * The data was increased by 100x.\n",
    " \n",
    "Our data would be stored in an Amazon S3 bucket (instead of storing it in the EMR cluster along with the staging tables) and loaded to our staging tables. We would still use Spark as it as our data processing platform since it is the best suited platform for very large datasets.\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " In this case we can use a batch job ochestration such as Apache Airflow to perform the ETL and data qualtiy validation.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " Once the data is ready to be consumed, it would be stored in a postgres database on a redshift cluster that easily supports multiuser access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
